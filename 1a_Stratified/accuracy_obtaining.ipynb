{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Class Accuracy Analysis\n",
      "==================================================\n",
      "\n",
      "Loading per-class accuracies from output folder...\n",
      "\n",
      "Calculating mean and standard deviation across folds...\n",
      "Saved statistics for FS_PCA_NN\n",
      "Saved statistics for FS_PCA_SVM\n",
      "Saved statistics for RF\n",
      "Saved statistics for FS_PCA_QDA\n",
      "Saved statistics for FS_PCA_KNN\n",
      "Saved statistics for FS_PCA_LR\n",
      "\n",
      "Creating formatted table...\n",
      "\n",
      "================================================================================\n",
      "PER-CLASS ACCURACY TABLE (mean ± std across 5 folds)\n",
      "================================================================================\n",
      "  Class            NN           SVM            RF           QDA           KNN            LR\n",
      "Class 1 0.891 ± 0.034 0.945 ± 0.017 0.929 ± 0.014 0.934 ± 0.014 0.940 ± 0.026 0.902 ± 0.027\n",
      "Class 2 0.782 ± 0.028 0.739 ± 0.057 0.767 ± 0.038 0.668 ± 0.067 0.739 ± 0.028 0.733 ± 0.052\n",
      "Class 3 0.903 ± 0.051 0.896 ± 0.049 0.873 ± 0.039 0.903 ± 0.055 0.843 ± 0.055 0.836 ± 0.060\n",
      "Class 4 0.856 ± 0.157 0.876 ± 0.143 0.774 ± 0.147 0.836 ± 0.085 0.835 ± 0.132 0.815 ± 0.120\n",
      "Class 5 0.879 ± 0.033 0.922 ± 0.017 0.905 ± 0.033 0.974 ± 0.021 0.957 ± 0.027 0.905 ± 0.033\n",
      "Class 6 0.800 ± 0.127 0.835 ± 0.101 0.788 ± 0.115 0.894 ± 0.024 0.729 ± 0.142 0.741 ± 0.096\n",
      "Class 7 0.950 ± 0.032 0.960 ± 0.037 0.950 ± 0.045 0.990 ± 0.020 0.960 ± 0.037 0.960 ± 0.049\n",
      "\n",
      "Table saved to per_class_accuracy_table.csv\n",
      "\n",
      "LaTeX table saved to per_class_accuracy_table.tex\n",
      "\n",
      "================================================================================\n",
      "DETAILED PER-CLASS ACCURACY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "FS_PCA_NN:\n",
      "----------------------------------------\n",
      "  Class 1: 0.8907 ± 0.0342\n",
      "  Class 2: 0.7815 ± 0.0277\n",
      "  Class 3: 0.9026 ± 0.0515\n",
      "  Class 4: 0.8558 ± 0.1570\n",
      "  Class 5: 0.8790 ± 0.0333\n",
      "  Class 6: 0.8000 ± 0.1267\n",
      "  Class 7: 0.9505 ± 0.0316\n",
      "  Overall mean: 0.8657\n",
      "  Overall std: 0.0660\n",
      "\n",
      "FS_PCA_SVM:\n",
      "----------------------------------------\n",
      "  Class 1: 0.9453 ± 0.0171\n",
      "  Class 2: 0.7394 ± 0.0573\n",
      "  Class 3: 0.8960 ± 0.0488\n",
      "  Class 4: 0.8763 ± 0.1427\n",
      "  Class 5: 0.9225 ± 0.0171\n",
      "  Class 6: 0.8353 ± 0.1012\n",
      "  Class 7: 0.9605 ± 0.0373\n",
      "  Overall mean: 0.8822\n",
      "  Overall std: 0.0602\n",
      "\n",
      "RF:\n",
      "----------------------------------------\n",
      "  Class 1: 0.9288 ± 0.0140\n",
      "  Class 2: 0.7672 ± 0.0378\n",
      "  Class 3: 0.8729 ± 0.0385\n",
      "  Class 4: 0.7737 ± 0.1471\n",
      "  Class 5: 0.9051 ± 0.0328\n",
      "  Class 6: 0.7882 ± 0.1153\n",
      "  Class 7: 0.9505 ± 0.0447\n",
      "  Overall mean: 0.8552\n",
      "  Overall std: 0.0615\n",
      "\n",
      "FS_PCA_QDA:\n",
      "----------------------------------------\n",
      "  Class 1: 0.9344 ± 0.0136\n",
      "  Class 2: 0.6685 ± 0.0668\n",
      "  Class 3: 0.9031 ± 0.0552\n",
      "  Class 4: 0.8358 ± 0.0848\n",
      "  Class 5: 0.9739 ± 0.0213\n",
      "  Class 6: 0.8941 ± 0.0235\n",
      "  Class 7: 0.9900 ± 0.0200\n",
      "  Overall mean: 0.8857\n",
      "  Overall std: 0.0408\n",
      "\n",
      "FS_PCA_KNN:\n",
      "----------------------------------------\n",
      "  Class 1: 0.9399 ± 0.0264\n",
      "  Class 2: 0.7394 ± 0.0283\n",
      "  Class 3: 0.8430 ± 0.0553\n",
      "  Class 4: 0.8353 ± 0.1318\n",
      "  Class 5: 0.9565 ± 0.0275\n",
      "  Class 6: 0.7294 ± 0.1422\n",
      "  Class 7: 0.9605 ± 0.0373\n",
      "  Overall mean: 0.8577\n",
      "  Overall std: 0.0641\n",
      "\n",
      "FS_PCA_LR:\n",
      "----------------------------------------\n",
      "  Class 1: 0.9018 ± 0.0274\n",
      "  Class 2: 0.7325 ± 0.0523\n",
      "  Class 3: 0.8359 ± 0.0599\n",
      "  Class 4: 0.8147 ± 0.1198\n",
      "  Class 5: 0.9051 ± 0.0328\n",
      "  Class 6: 0.7412 ± 0.0956\n",
      "  Class 7: 0.9600 ± 0.0490\n",
      "  Overall mean: 0.8416\n",
      "  Overall std: 0.0624\n",
      "\n",
      "================================================================================\n",
      "SUMMARY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Best model for each class (based on mean accuracy):\n",
      "  Class 1: SVM (0.945)\n",
      "  Class 2: NN (0.782)\n",
      "  Class 3: QDA (0.903)\n",
      "  Class 4: SVM (0.876)\n",
      "  Class 5: QDA (0.974)\n",
      "  Class 6: QDA (0.894)\n",
      "  Class 7: QDA (0.990)\n",
      "\n",
      "Model stability ranking (based on average std across classes):\n",
      "  1. QDA: 0.0408\n",
      "  2. SVM: 0.0602\n",
      "  3. RF: 0.0615\n",
      "  4. LR: 0.0624\n",
      "  5. KNN: 0.0641\n",
      "  6. NN: 0.0660\n",
      "\n",
      "Class difficulty ranking (based on average accuracy across all models):\n",
      "  1. Class 7: 0.962\n",
      "  2. Class 5: 0.924\n",
      "  3. Class 1: 0.923\n",
      "  4. Class 3: 0.876\n",
      "  5. Class 4: 0.832\n",
      "  6. Class 6: 0.798\n",
      "  7. Class 2: 0.738\n",
      "\n",
      "================================================================================\n",
      "Analysis complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Per-Class Accuracy Analysis Script\n",
    "Analyzes the per-class accuracy for each model across all test folds\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_ROOT = 'output'\n",
    "METRICS_DIR = os.path.join(OUTPUT_ROOT, 'metrics')\n",
    "N_FOLDS = 5  # Number of CV folds\n",
    "N_CLASSES = 7  # Number of classes\n",
    "MODEL_SET = ['FS_PCA_NN', 'FS_PCA_SVM', 'RF', 'FS_PCA_QDA', 'FS_PCA_KNN', 'FS_PCA_LR']\n",
    "\n",
    "# Model display names for better formatting\n",
    "MODEL_DISPLAY_NAMES = {\n",
    "    'FS_PCA_NN': 'NN',\n",
    "    'FS_PCA_SVM': 'SVM', \n",
    "    'RF': 'RF',\n",
    "    'FS_PCA_QDA': 'QDA',\n",
    "    'FS_PCA_KNN': 'KNN',\n",
    "    'FS_PCA_LR': 'LR'\n",
    "}\n",
    "\n",
    "def load_per_class_accuracies():\n",
    "    \"\"\"\n",
    "    Load per-class accuracies (recall) for all models and folds\n",
    "    Note: Per-class recall = per-class accuracy (TP / (TP + FN))\n",
    "    \"\"\"\n",
    "    all_accuracies = {}\n",
    "    \n",
    "    for model_name in MODEL_SET:\n",
    "        model_accuracies = []\n",
    "        \n",
    "        for fold_idx in range(1, N_FOLDS + 1):\n",
    "            # Load per-class recall which is equivalent to per-class accuracy\n",
    "            filename = f\"per_class_recall_{model_name}_{fold_idx}.npy\"\n",
    "            filepath = os.path.join(METRICS_DIR, filename)\n",
    "            \n",
    "            if os.path.exists(filepath):\n",
    "                per_class_acc = np.load(filepath)\n",
    "                model_accuracies.append(per_class_acc)\n",
    "            else:\n",
    "                print(f\"Warning: File not found - {filename}\")\n",
    "        \n",
    "        if model_accuracies:\n",
    "            # Stack all folds: shape (n_folds, n_classes)\n",
    "            all_accuracies[model_name] = np.array(model_accuracies)\n",
    "        else:\n",
    "            print(f\"Error: No data found for model {model_name}\")\n",
    "    \n",
    "    return all_accuracies\n",
    "\n",
    "def calculate_statistics(all_accuracies):\n",
    "    \"\"\"\n",
    "    Calculate mean and std for each model across all folds\n",
    "    \"\"\"\n",
    "    statistics = {}\n",
    "    \n",
    "    for model_name, accuracies in all_accuracies.items():\n",
    "        # accuracies shape: (n_folds, n_classes)\n",
    "        mean_acc = np.mean(accuracies, axis=0)  # Shape: (n_classes,)\n",
    "        std_acc = np.std(accuracies, axis=0)    # Shape: (n_classes,)\n",
    "        \n",
    "        statistics[model_name] = {\n",
    "            'mean': mean_acc,\n",
    "            'std': std_acc\n",
    "        }\n",
    "        \n",
    "        # Save as numpy arrays\n",
    "        np.save(f\"{model_name}_per_class_acc_mean.npy\", mean_acc)\n",
    "        np.save(f\"{model_name}_per_class_acc_std.npy\", std_acc)\n",
    "        print(f\"Saved statistics for {model_name}\")\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "def create_formatted_table(statistics):\n",
    "    \"\"\"\n",
    "    Create a formatted table showing mean ± std for each class and model\n",
    "    \"\"\"\n",
    "    # Create empty dataframe\n",
    "    table_data = []\n",
    "    \n",
    "    # For each class (row)\n",
    "    for class_idx in range(N_CLASSES):\n",
    "        row_data = {'Class': f'Class {class_idx + 1}'}\n",
    "        \n",
    "        # For each model (column)\n",
    "        for model_name in MODEL_SET:\n",
    "            mean_val = statistics[model_name]['mean'][class_idx]\n",
    "            std_val = statistics[model_name]['std'][class_idx]\n",
    "            \n",
    "            # Format as \"mean ± std\" with 3 decimal places\n",
    "            formatted_value = f\"{mean_val:.3f} ± {std_val:.3f}\"\n",
    "            \n",
    "            # Use display name for column\n",
    "            display_name = MODEL_DISPLAY_NAMES[model_name]\n",
    "            row_data[display_name] = formatted_value\n",
    "        \n",
    "        table_data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_detailed_statistics(statistics):\n",
    "    \"\"\"\n",
    "    Print detailed statistics for verification\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED PER-CLASS ACCURACY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_name in MODEL_SET:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        mean_acc = statistics[model_name]['mean']\n",
    "        std_acc = statistics[model_name]['std']\n",
    "        \n",
    "        for class_idx in range(N_CLASSES):\n",
    "            print(f\"  Class {class_idx + 1}: {mean_acc[class_idx]:.4f} ± {std_acc[class_idx]:.4f}\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"  Overall mean: {np.mean(mean_acc):.4f}\")\n",
    "        print(f\"  Overall std: {np.mean(std_acc):.4f}\")\n",
    "\n",
    "def save_latex_table(df, filename='per_class_accuracy_table.tex'):\n",
    "    \"\"\"\n",
    "    Save the table in LaTeX format\n",
    "    \"\"\"\n",
    "    latex_table = df.to_latex(index=False, escape=False)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    print(f\"\\nLaTeX table saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main analysis function\n",
    "    \"\"\"\n",
    "    print(\"Per-Class Accuracy Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load per-class accuracies\n",
    "    print(\"\\nLoading per-class accuracies from output folder...\")\n",
    "    all_accuracies = load_per_class_accuracies()\n",
    "    \n",
    "    if not all_accuracies:\n",
    "        print(\"Error: No data loaded. Please check the output folder.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"\\nCalculating mean and standard deviation across folds...\")\n",
    "    statistics = calculate_statistics(all_accuracies)\n",
    "    \n",
    "    # Create formatted table\n",
    "    print(\"\\nCreating formatted table...\")\n",
    "    table_df = create_formatted_table(statistics)\n",
    "    \n",
    "    # Display table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-CLASS ACCURACY TABLE (mean ± std across {} folds)\".format(N_FOLDS))\n",
    "    print(\"=\"*80)\n",
    "    print(table_df.to_string(index=False))\n",
    "    \n",
    "    # Save table to CSV\n",
    "    csv_filename = 'per_class_accuracy_table.csv'\n",
    "    table_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\nTable saved to {csv_filename}\")\n",
    "    \n",
    "    # Save LaTeX version\n",
    "    save_latex_table(table_df)\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print_detailed_statistics(statistics)\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find best model for each class\n",
    "    print(\"\\nBest model for each class (based on mean accuracy):\")\n",
    "    for class_idx in range(N_CLASSES):\n",
    "        best_acc = -1\n",
    "        best_model = \"\"\n",
    "        \n",
    "        for model_name in MODEL_SET:\n",
    "            mean_acc = statistics[model_name]['mean'][class_idx]\n",
    "            if mean_acc > best_acc:\n",
    "                best_acc = mean_acc\n",
    "                best_model = MODEL_DISPLAY_NAMES[model_name]\n",
    "        \n",
    "        print(f\"  Class {class_idx + 1}: {best_model} ({best_acc:.3f})\")\n",
    "    \n",
    "    # Find most stable model (lowest average std)\n",
    "    print(\"\\nModel stability ranking (based on average std across classes):\")\n",
    "    stability_scores = []\n",
    "    \n",
    "    for model_name in MODEL_SET:\n",
    "        avg_std = np.mean(statistics[model_name]['std'])\n",
    "        stability_scores.append((MODEL_DISPLAY_NAMES[model_name], avg_std))\n",
    "    \n",
    "    stability_scores.sort(key=lambda x: x[1])\n",
    "    \n",
    "    for rank, (model, avg_std) in enumerate(stability_scores, 1):\n",
    "        print(f\"  {rank}. {model}: {avg_std:.4f}\")\n",
    "    \n",
    "    # Class difficulty analysis\n",
    "    print(\"\\nClass difficulty ranking (based on average accuracy across all models):\")\n",
    "    class_difficulties = []\n",
    "    \n",
    "    for class_idx in range(N_CLASSES):\n",
    "        class_accs = []\n",
    "        for model_name in MODEL_SET:\n",
    "            class_accs.append(statistics[model_name]['mean'][class_idx])\n",
    "        avg_acc = np.mean(class_accs)\n",
    "        class_difficulties.append((f\"Class {class_idx + 1}\", avg_acc))\n",
    "    \n",
    "    class_difficulties.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for rank, (class_name, avg_acc) in enumerate(class_difficulties, 1):\n",
    "        print(f\"  {rank}. {class_name}: {avg_acc:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Analysis complete!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
