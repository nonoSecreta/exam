1a

**At the very beginning, we should make sure that each sample in the data used in the pipeline has their own id(related to their row index in the raw data file) which would be neglected during all model fitting and predicting but would be 
used to record and make the experiment tractable.**
**use relative path + env var(like DATA_ROOT)avoid absolute paths**


Six models: FS+PCA+NN, FS+PCA+SVM, RF(do not use dim reduction and FS!), FS+PCA+QDA, FS+PCA+KNN, FS+PCA+LR (FS: feature selection: filter method with F: f_classif)
figure that need to be saved(each of them should have their own directory):
    All of these metrics:Accuracy / Balanced Accuracy / Macro-F1 and "Per-class F1 / Recall / Specificity / Precision" for each test folder, name: {matric_name}_{model_name}_{test_folder_index} (note that Per-class F1 / Recall / Specificity / Precision should be over all label classes for each model at each outer fold)
    for each folder index i, save all indexes(the id of the sample) of test samples: test_{i}[I], and the best paras for each model. name: test_index_{test_folder_index}, best_para_{model_name}_{test_folder_index}
    all the plots and the data used by the plot. name: {plot_name}_{data_name}(note! one directory for one type of plot)
    Except of SVM, all the predict probability vector of the output of each model for each test samples: saved as np array pred_dist_{model_name}_{test_folder_index} each row: sample id | pred dist
    for each of all test fold, save the predict result for each model at each sample. name: pred_res_{model_name}_{test_folder_index} each row: sample id | pred result

Coding:

    global var:

        random seed

        data_path

        N_f # num of folders of CV

        model_set # set of models to be compared

        set_of_para # set of hyperpara for each models(note that unless RF, other models should contain K:#num of features to be retained after filter method and N_dim:# dimension of features to be retained after dimension reduction PCA,
        also note that para of K = dim_feat and skipping PCA should be allowed)(also also note that, you should do PCA on training data set, and then apply the obtained PCA on test and val data set, so as feature selection)
        
        for ease of maintenance, all string-type variables(like the path names, the file names, plot title name) should be placed at the beginning together with global variables


    data var:
        dim_feat #original dimension of features
        num_labels #num of labels







    pipeline:

        data_import
        data_cleaning
        feature nomalization
        data_split:(select one of the approaches)
             1. randomly choosing 170 samples from each class as data(since the class with least samples has 170 samples while the largest one has 363, selecting 170 would not loss too much info)
             2. don't do anything




        double CV(use sklearn.model_selection ParameterSampler):(at each fitting, this pipeline should be followed:
        feature selection(not for RF) \to PCA(not for RF) \to fitting)
            outer folder: i \in [1,...,N_f]
                data[block[i]] as test data(test)
                data[block[(-i)]] as training-validate data(tr-val) # block[(-i)] means all blocks of data with indexes != i
                inner folder: j \in [1, ..., N_f]
                    tr-val[block[j]] as val data
                    tr-val[block[(-j)]] as tr data
                    model index: k \in [1, ..., N_model]
                        para : S \in set_of_para(this step, use sklearn.model_selection ParameterSampler)
                            fit(model(para[S])[k], tr)
                            prd = predict(model(para[l])[k], val)
                            score[j][k][S] = eval(prd, val) # balance accuracy
                            
                mean_score = mean(score, axis = j) 

                model index: k \in [1, ..., N_model] 
                    best_para = argmax_S(mean_score, k)
                    fit(model(best_para)[k], tr-val)
                    prd = predict(model(best_para)[k], test)
                    test_score[i][k] = eval(prd, test) # All of these metrics: accuracy, F1, balance accuracy, Macro-Averaged Metrics
            mean_test_score = mean(test_score, axis = i)


    visualization:

        table of the mean and standard deviation of each metric per class:(Per-class F1 / Recall / Specificity / Precision)

        box plot of all metrics across all N_f outer folder test set
            3 plot (for 3 metrics:Accuracy / Balanced Accuracy / Macro-F1)
            x-axis: {model name}
            y-axis: metrics score obtained at each test folder
            title: Boxplot of {metrics name} distributions of each model across {N_f} CV tests

        Confusion matrix:
            1 plot
            First, average all folders' confusion matrices.
            Then, normalize each row to convert values into proportions.
            Finally, visualize the result using a heatmap.
            title: Row-normalized mean confusion matrix over {N_f} tests

        box plot of dist of Maximum predicted probability for each model across all test folder
            5 plot(for 5 models, we do not accept the Max pred prob of SVM)
            x-axis: {test_folder_index}
            y-axis: {Maximum predicted probability for each samples}
            title: Distribution of Prediction Confidence for {model name} across {N_f} tests

        Calibration curve for each model at each fold

